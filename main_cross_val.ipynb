{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Perceptron, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def impute(df, label_column='GROUP'):\n",
    "    imputed_dfs = []\n",
    "    for group_value in df[label_column].unique().tolist():\n",
    "        # Extract the non-numeric column and the numeric columns\n",
    "        non_numeric_column = df[df[label_column] == group_value].iloc[:, :2].reset_index(drop=True)\n",
    "        numeric_data = df[df[label_column] == group_value].iloc[:, 2:].reset_index(drop=True)\n",
    "\n",
    "        # Initialize the KNNImputer\n",
    "        knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "        # Impute the missing values in the numeric data\n",
    "        imputed_numeric_data = knn_imputer.fit_transform(numeric_data)\n",
    "\n",
    "        # Create a new DataFrame with the imputed values\n",
    "        imputed_data_frame = pd.DataFrame(imputed_numeric_data, columns=numeric_data.columns)\n",
    "\n",
    "        imputed_data_frame = pd.concat([non_numeric_column, imputed_data_frame], axis=1)\n",
    "        imputed_dfs.append(imputed_data_frame)\n",
    "\n",
    "    return pd.concat(imputed_dfs).reset_index(drop=True)\n",
    "\n",
    "df_Turkish_meta = pd.read_csv('../src/Turkish meta.txt', sep='\\t')\n",
    "df_Turkish_data = pd.read_csv('../src/Turkish data.txt', sep='\\t')\n",
    "df_Swedish_meta = pd.read_csv('../src/Swedish meta.txt', sep='\\t')\n",
    "df_Swedish_data = pd.read_csv('../src/Swedish data.txt', sep='\\t')\n",
    "\n",
    "df_Turkish = df_Turkish_meta.merge(df_Turkish_data, on='DAid')\n",
    "df_Swedish = df_Swedish_meta.merge(df_Swedish_data, on='DAid')\n",
    "\n",
    "df_combined = pd.concat([df_Turkish, df_Swedish], axis=0).reset_index(drop=True)\n",
    "\n",
    "df_combined_imputed = impute(df_combined)\n",
    "\n",
    "Disease_list = ['LIVD-Chronic Liver Disease (CLD)', 'LIVD-ARLD', \n",
    "                'LIVD-Hepatocellular Carcinoma (HCC)', 'LIVD-VIRAL', 'LIVD-MASLD', \n",
    "                'PANC-pancreas cancer', 'PSYC-bipolar', 'PSYC-schizophrenia', 'THEL- ', 'COLC- ', \n",
    "                'AML', 'BRC', 'CLL', 'CVX', 'ENDC', 'GLIOM', 'LUNGC', 'LYMPH', 'MENI', 'MYEL', 'OVC', 'PIT-NET', \n",
    "                'PRC', 'SI-NET']\n",
    "\n",
    "disease_mapping = {disease: i for i, disease in enumerate(Disease_list)}\n",
    "\n",
    "df_train_test_cohort = df_combined_imputed[df_combined_imputed['GROUP'].isin(Disease_list)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "x, y = df_train_test_cohort.iloc[:,2:].values, np.array([disease_mapping[disease] for disease in df_train_test_cohort['GROUP'].tolist()], dtype=int)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the IDs for each category group\n",
    "blood_id = [8, 10, 12, 17, 19]\n",
    "liver_id = [0, 1, 2, 3, 4]\n",
    "psych_id = [6, 7]\n",
    "cancer_id = [5, 9, 11, 13, 14, 15, 16, 18, 20, 21, 22, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('clf',\n",
      "                 Perceptron(alpha=1e-07, n_jobs=-1, penalty='l1',\n",
      "                            random_state=3))])\n",
      "F1: 0.9841709976143244\n"
     ]
    }
   ],
   "source": [
    "def _create_first_layer_labels(y):\n",
    "    # Create labels for the first layer\n",
    "    labels = np.full_like(y, -1, dtype=int)  # Default label for safety\n",
    "    labels[np.isin(y, blood_id)] = 0\n",
    "    labels[np.isin(y, liver_id)] = 1\n",
    "    labels[np.isin(y, psych_id)] = 2\n",
    "    labels[np.isin(y, cancer_id)] = 3\n",
    "    return labels\n",
    "\n",
    "y_first_layer = _create_first_layer_labels(y_train)\n",
    "\n",
    "sm = BorderlineSMOTE(k_neighbors=6, m_neighbors=8, n_jobs=-1, random_state=3)\n",
    "x_res, y_res = sm.fit_resample(x_train, y_first_layer)\n",
    "\n",
    "multi_classifier = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', Perceptron(random_state=3, n_jobs=-1)),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__penalty': [None, 'l1', 'l2'],\n",
    "    'clf__alpha': [1e-7, 1e-8, 1e-9]\n",
    "}\n",
    "\n",
    "tuned_multi_classifier = GridSearchCV(multi_classifier, param_grid, scoring='f1_weighted', cv=5).fit(x_res, y_res)\n",
    "    \n",
    "print(tuned_multi_classifier.best_estimator_)\n",
    "print(\"F1:\", f1_score(_create_first_layer_labels(y_test), tuned_multi_classifier.predict(x_test), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('clf', ExtraTreesClassifier(n_jobs=-1, random_state=3))])\n",
      "F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "blood_classifier = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', ExtraTreesClassifier(random_state=3, n_jobs=-1)),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [75, 100, 125],\n",
    "    'clf__max_depth': [None, 5, 7]\n",
    "}\n",
    "\n",
    "mask = np.isin(y_train, blood_id)\n",
    "sm = BorderlineSMOTE(k_neighbors=4, m_neighbors=13, n_jobs=-1, random_state=3)\n",
    "x_res, y_res = sm.fit_resample(x_train[mask], y_train[mask])\n",
    "\n",
    "tuned_blood_classifier = GridSearchCV(blood_classifier, param_grid, scoring='f1_weighted', cv=5).fit(x_res, y_res)\n",
    "    \n",
    "print(tuned_blood_classifier.best_estimator_)\n",
    "print(\"F1:\", f1_score(y_test[np.isin(y_test, blood_id)], tuned_blood_classifier.predict(x_test[np.isin(y_test, blood_id)]), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('clf',\n",
      "                 RidgeClassifier(alpha=0.1, random_state=3, solver='svd'))])\n",
      "F1: 0.7522884233821734\n"
     ]
    }
   ],
   "source": [
    "liver_classifier = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', RidgeClassifier(random_state=3)),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__alpha': [0.1, 1, 10],\n",
    "    'clf__solver': ['svd', 'cholesky', 'lsqr']\n",
    "}\n",
    "\n",
    "mask = np.isin(y_train, liver_id)\n",
    "sm = BorderlineSMOTE(k_neighbors=9, m_neighbors=7, n_jobs=-1, random_state=3)\n",
    "x_res, y_res = sm.fit_resample(x_train[mask], y_train[mask])\n",
    "\n",
    "tuned_liver_classifier = GridSearchCV(liver_classifier, param_grid, scoring='f1_weighted', cv=5).fit(x_res, y_res)\n",
    "    \n",
    "print(tuned_liver_classifier.best_estimator_)\n",
    "print(\"F1:\", f1_score(y_test[np.isin(y_test, liver_id)], tuned_liver_classifier.predict(x_test[np.isin(y_test, liver_id)]), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('clf', SGDClassifier(alpha=5e-05, n_jobs=-1, random_state=3))])\n",
      "F1: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "psych_classifier = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', SGDClassifier(random_state=3, n_jobs=-1)),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__loss': ['hinge', 'squared_error', 'perceptron'],\n",
    "    'clf__penalty': ['l2', 'l1', None],\n",
    "    'clf__alpha': [5e-5, 1e-4, 2e-4]\n",
    "}\n",
    "\n",
    "mask = np.isin(y_train, psych_id)\n",
    "sm = BorderlineSMOTE(k_neighbors=4, m_neighbors=3, n_jobs=-1, random_state=3)\n",
    "x_res, y_res = sm.fit_resample(x_train[mask], y_train[mask])\n",
    "\n",
    "tuned_psych_classifier = GridSearchCV(psych_classifier, param_grid, scoring='f1_weighted', cv=5).fit(x_res, y_res)\n",
    "    \n",
    "print(tuned_psych_classifier.best_estimator_)\n",
    "print(\"F1:\", f1_score(y_test[np.isin(y_test, psych_id)], tuned_psych_classifier.predict(x_test[np.isin(y_test, psych_id)]), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('clf',\n",
      "                 LogisticRegression(C=0.01, n_jobs=-1, random_state=3,\n",
      "                                    solver='liblinear'))])\n",
      "F1: 0.8051238454657911\n"
     ]
    }
   ],
   "source": [
    "cancer_classifier = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(random_state=3, n_jobs=-1)),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__penalty': ['l1', 'l2', None],\n",
    "    'clf__C': [0.001, 0.005, 0.01],\n",
    "    'clf__solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "mask = np.isin(y_train, cancer_id)\n",
    "sm = BorderlineSMOTE(k_neighbors=1, m_neighbors=10, n_jobs=-1, random_state=3)\n",
    "x_res, y_res = sm.fit_resample(x_train[mask], y_train[mask])\n",
    "\n",
    "tuned_cancer_classifier = GridSearchCV(cancer_classifier, param_grid, scoring='f1_weighted', cv=5).fit(x_res, y_res)\n",
    "    \n",
    "print(tuned_cancer_classifier.best_estimator_)\n",
    "print(\"F1:\", f1_score(y_test[np.isin(y_test, cancer_id)], tuned_cancer_classifier.predict(x_test[np.isin(y_test, cancer_id)]), average='weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Proteomics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
